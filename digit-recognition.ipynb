{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Digit Recognition Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author: Kevin Delassus - G00270791"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notebook Purpose\n",
    "The purpose of this notebook is to explain how the script file [digitrec.py](http://localhost:8888/edit/digitrec.py) works and also explain its performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There can be many challanges assoiated to detecting hand written digits. If we take a human for example, humans and effortly recognize digits due to humans having a primary cortex in each hemisphere of our brain. Each primary cortex contains 140 million neurons and tens of billons of connections yet human vision involves not just primary cortex, but an entire series of visual cortices doing progressively more complex image processing. Recognizing handwritten digits isn't easy. Rather, we humans are stupendously, astoundingly good at making sense of what our eyes show us. But nearly all that work is done unconsciously. And so we don't usually appreciate how tough a problem our visual systems solve.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](https://alleninstitute.org/media/filer_public/74/44/74443675-6280-49a1-8362-61cecb90681c/neurons_all_16_large_blackbg-reid.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Programming a Neural Networks makes this problem of digit detection easier to solve. Similar to how a child learns to recognise objects, we need to show an algorithm millions of pictures of different digits before it is be able to generalize the input and make predictions for images it has never seen before.\n",
    "\n",
    "Computers see images in a different way than humans do. They can only see numbers. Every image can be represented as 2-dimensional arrays of numbers, known as pixels. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](https://cdn-images-1.medium.com/max/1600/1*ccVO7341XIh7GfvzQS1IGw.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I used Keras to create my neural network. [Keras](https://keras.io/) is a high-level neural networks API, written in Python and capable of running on top of TensorFlow. Another great reason for using Keras is that it already contains the MNIST dataset meaning we don't need to go de-compress the train and test files.\n",
    "\n",
    "I used a Convolutional Neural Networks approach opposed to using simple Neural Network due to it being more accurate. This will be explained in more detail below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Convolutional Neural Network is a class of deep, feed-forward artificial neural networks, most commonly applied to analyzing visual imagery.\n",
    "\n",
    "Convolutional Neural Networks have a different architecture than regular Neural Networks. Regular Neural Networks transform an input by putting it through a series of hidden layers. Every layer is made up of a set of neurons, where each layer is fully connected to all neurons in the layer before. Finally, there is a last fully-connected layer — the output layer — that represent the predictions.\n",
    "\n",
    "Convolutional Neural Networks are a bit different. First of all, the layers are organised in 3 dimensions: width, height and depth. Further, the neurons in one layer do not connect to all the neurons in the next layer but only to a small region of it. Lastly, the final output will be reduced to a single vector of probability scores, organized along the depth dimension."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### digitrec.py Explained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Please note that the code snippets below are not intended for running. Certain parts of the code have been removed to be able to explain the core parts of the script. If you intend on running the code please use the digitrec.py script file.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Structure\n",
    "I wanted to structure the script so that you did not need to keep re-training the model. So when you start the script it asks you whether you wish to train a model on the MNIST dataset or test an existing saved model. This is done by saving the model in json. If you decide to train a model then script it will automaticly go into the test phase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cv2'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-822f7982f832>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtkinter\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmodel_from_json\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'cv2'"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import keras as kr\n",
    "from keras.datasets import mnist\n",
    "from tkinter import filedialog\n",
    "from tkinter import *\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import model_from_json\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading MNIST Dataset\n",
    "The step below loads the MNIST dataset into different arrays. train_img which contain all the training images, train_lbl which contains all the training labels, test_img which contains all the test images and test_lbl which contains all the test labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data from keras.datasets\n",
    "(train_img, train_lbl), (test_img, test_lbl) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reshaping data\n",
    "The datasets are 3D arrays. Training dataset shape is (60000, 28, 28) & Testing dataset shape is (10000, 28, 28).\n",
    "The input shape that CNN expects is a 4D array (batch, height, width, channels). Channels signify whether the image is grayscale or colored. In our case, we are using grayscale images so we give 1 for channels if these are colored images we give 3(RGB). Below code for reshaping our inputs.\n",
    "\n",
    "We are also one hot encoding the train and test images & labels. Our Datasets will have data in each pixel in between 0–255 so now we scale it to 0–1 using below code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape to the expected CNN format \n",
    "train_img = train_img.reshape(train_img.shape[0], train_img.shape[1], train_img.shape[2], 1).astype('float32')\n",
    "test_img = test_img.reshape(test_img.shape[0], test_img.shape[1], test_img.shape[2], 1).astype('float32')\n",
    "\n",
    "# One hot encode train_img & test_img\n",
    "train_img/=255\n",
    "test_img/=255\n",
    "\n",
    "# one hot encode\n",
    "train_lbl = kr.utils.to_categorical(train_lbl, 10)\n",
    "test_lbl = kr.utils.to_categorical(test_lbl, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building the Convolutional Neural Network\n",
    "Below I am building a Convolutional Neural Network.\n",
    "- The first layer of code is a hidden layer called a Convolution2D. The layer has 32 filters/output channels, which with the size of 5×5 and an activation function. This is the input layer, expecting images with the structure outlined above (height, width, channels).\n",
    "- The Second layer is the MaxPooling layer. MaxPooling layer is used to down-sample the input to enable the model to make assumptions about the features so as to reduce over-fitting. It also reduces the number of parameters to learn, reducing the training time.\n",
    "- The third layer is a hidden layer with 32 filters/output channels with the size of 3×3 and an activation function.\n",
    "- The Forth layer is a MaxPooling layer.\n",
    "- The Firth layer is a regularization layer using dropout called Dropout. It is configured to randomly exclude 20% of neurons in the layer in order to reduce overfitting.\n",
    "- The sixth layer converts the 2D matrix data to a vector called Flatten. It allows the output to be processed by standard fully connected layers.\n",
    "- The seventh is a fully connected layer with 128 neurons.\n",
    "- The eighth and final layer is a output layer with 10 neurons and it uses softmax activation function. Each neuron will give the probability of that class. It’s a multi-class classification that’s why softmax activation function if it was a binary classification we use sigmoid activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = kr.models.Sequential()\n",
    "\n",
    "model.add(kr.layers.convolutional.Conv2D(32, (5, 5), input_shape=(train_img.shape[1], train_img.shape[2], 1), activation='relu'))\n",
    "model.add(kr.layers.convolutional.MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(kr.layers.convolutional.Conv2D(32, (3, 3), activation='relu'))\n",
    "model.add(kr.layers.convolutional.MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(kr.layers.Dropout(0.2))\n",
    "model.add(kr.layers.Flatten())\n",
    "model.add(kr.layers.Dense(128, activation='relu'))\n",
    "model.add(kr.layers.Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compiling the Model\n",
    "To complile the model I used categorical_crossentropy as a loss function because its a multi-class classification problem. I used Adam as Optimizer to make sure our weights optimized properly. I used accuracy as metrics to improve the performance of our neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiling of the Model\n",
    "model.compile(loss='categorical_crossentropy', optimizer=kr.optimizers.Adam(), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the Model\n",
    "The model is going to fit over the user defined epochs and updates after every 200 images training. For the notebook we are going to specify the epochs to 10. The test data is used as the validation dataset, allowing you to see the skill of the model as it trains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model\n",
    "epochsNum=10\n",
    "model.fit(train_img, train_lbl, validation_data=(test_img, test_lbl), epochs=epochsNum, batch_size=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Store Model\n",
    "I decided it would be a good idea to store the model on file. This give the user the option to reuse the model without having to re-train. To store the model I am using kera's model to JSON library. Two files are created. One a model.json and two a model.h5 file which is a [HDF5.](https://support.hdfgroup.org/HDF5/whatishdf5.html) Both files are stored within a folder called models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# serialize model to JSON\n",
    "model_json = model.to_json()\n",
    "with open(\"models/model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"models/model.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation of the Model\n",
    "The test dataset is used to evaluate the model and after evaluation Test loss & Test Accuracy metrics will be printed. I actived a 99% accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation of the model\n",
    "metrics = model.evaluate(test_img, test_lbl, verbose=0)\n",
    "print(\"Metrics(Test loss & Test Accuracy): \")\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Making Predictions\n",
    "I wanted to be able to test my own hand writen digits efficiently without having to change the script code. To do this I used [tkinter](https://wiki.python.org/moin/TkInter) to create a gui to be able to select the desired image to be tested. The image below is a snap of the GUI. To create the test images I used [GIMP](https://www.gimp.org/) which is a free open source raster graphics editor used for image retouching and editing, free-form drawing, converting between different image formats, and more specialized tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](notebook-images/ImageSelectBox.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Saved Model\n",
    "I am going to do the opposite of what we did to save the model. First I loaded the the model.json and used that to create a model. Then I loaded the weights into the new model.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # load json and create model\n",
    "json_file = open('models/model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(\"models/model.h5\")\n",
    "print(\"Loaded model from disk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Open Dialog Box\n",
    "Using thinter I created a dialog to select an image to try and predict. The dialog can only accept a jpeg file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Asking user to enter own image for testing\n",
    "root = Tk()\n",
    "root.testImage =  filedialog.askopenfilename(initialdir = \"C:\\\\\",title = \"Select Image\",filetypes = ((\"jpeg files\",\"*.jpg\"),(\"all files\",\"*.*\")))\n",
    "print(root.testImage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read Image and Resize\n",
    "The selected image is then read in and sized now to 28x28 using openCV. \n",
    "Like the train and test images, the predict image needs to be reshaped to the expected CNN format.\n",
    "The last set in preparing the image before prediction is to one hot encode.\n",
    "The image can now be passed in to be predicted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading image and resizing it to correct CNN format\n",
    "imgFile = cv2.imread(root.testImage)\n",
    "img = cv2.resize(imgFile, (28, 28))\n",
    "arr = img.reshape(-1,28, 28, 1).astype('float32')\n",
    "\n",
    "# One hot encode arr\n",
    "arr/=255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make Prediction on Image\n",
    "Below we are making the prediction by using the model that was loaded from file, calling the predict_classes and passing in the images. A result is then stored in a prediction variable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'loaded_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-70782ed448b2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Making prediction\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloaded_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_classes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mprediction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'loaded_model' is not defined"
     ]
    }
   ],
   "source": [
    "# Making prediction\n",
    "result = loaded_model.predict_classes(arr)\n",
    "prediction = result[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Displaying Result\n",
    "Finally the selected image is displayed using matplotlib.pyplot and the predicticted result is then shown above the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying prediction\n",
    "print(\"Class: \",prediction)\n",
    "\n",
    "# Showing image and predicted result\n",
    "plt.imshow(imgFile)\n",
    "plt.title(prediction)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Previous Result\n",
    "![alt text](notebook-images/result.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To conclude on this notebook, Convolutional Neural Networks are definatly one the best for creating image classification and recognition software. If offers higher performance compared to other types of neural network models. A tradition model is expected to give an average of 9% when trained on the MNIST dataset compared to 99% on a Convolutional Neural Network. This is a huge difference. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### END"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
